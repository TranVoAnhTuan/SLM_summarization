# Fine-tuning Small Language Model (SLM) for Text Summarization 
> This models under 7 billion parameters that retain the transformer architecture while requiring drastically fewer resources. Fine-tuning such models with parameter-efficient methods can deliver competitive performance while maintaining low memory usage and fast inference, offering a promising alternative for edge deployment.
